{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/julia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "set_seed(123)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_token(encoding, q_label, r_label, q_reidx, r_reidx):\n",
    "    labels = []\n",
    "    labels_mask = []\n",
    "    for idx, (q_ls, q_r, r_ls, r_r) in enumerate(zip(q_label, q_reidx, r_label, r_reidx)):\n",
    "        word_idx = encoding.word_ids(batch_index= idx)\n",
    "        label = np.array([0 for x in word_idx])\n",
    "        label_mask = np.array([0 if x == None else 1 for x in word_idx])\n",
    "        if q_ls == [] or r_ls == []:\n",
    "            labels.append(label)\n",
    "            labels_mask.append(label_mask)\n",
    "            continue\n",
    "\n",
    "        for q_l in q_ls:\n",
    "            q_s, q_e =  q_l[0], q_l[1]\n",
    "\n",
    "            q_s = encoding.char_to_token(idx, q_s-q_r[0], 0)\n",
    "            q_e = encoding.char_to_token(idx, q_e-q_r[0], 0)\n",
    "            \n",
    "            if q_s == None or q_e == None:\n",
    "                continue\n",
    "\n",
    "            for i in range(q_s, q_e+1):\n",
    "                label[i] = 1\n",
    "                if i == 0 or word_idx[i] != word_idx[i-1]:\n",
    "                    pass\n",
    "                else: \n",
    "                    label_mask[i] = 0\n",
    "\n",
    "        for r_l in r_ls:\n",
    "            r_s, r_e =  r_l[0], r_l[1]\n",
    "\n",
    "            r_s = encoding.char_to_token(idx, r_s-r_r[0], 1)\n",
    "            r_e = encoding.char_to_token(idx, r_e-r_r[0], 1)\n",
    "            \n",
    "            if r_s == None or r_e == None:\n",
    "                continue\n",
    "            \n",
    "            for i in range(r_s, r_e+1):\n",
    "                label[i] = 1\n",
    "                if i == 0 or word_idx[i] != word_idx[i-1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    label_mask[i] = 0\n",
    "\n",
    "        labels.append(label)\n",
    "        labels_mask.append(label_mask)\n",
    "    return labels, labels_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_pos(train_encoding):\n",
    "    input_token_pos_list = []\n",
    "    input_token_pos = []\n",
    "    for i in range(len(train_encoding[\"input_ids\"])):\n",
    "        for id in train_encoding.word_ids(i):\n",
    "            if id == None:\n",
    "                input_token_pos.append(0)\n",
    "            else:\n",
    "                input_token_pos.append(1)\n",
    "        input_token_pos_list.append(input_token_pos)\n",
    "        input_token_pos = []\n",
    "    return input_token_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import *\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "\n",
    "class Mymodel(BertPreTrainedModel):\n",
    "    def __init__(self, config, lstm_hidden_size, lstm_dropout_prob, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        # lstm\n",
    "        # self.bilstm = nn.LSTM(config.hidden_size, lstm_hidden_size, dropout=lstm_dropout_prob, batch_first=True, bidirectional=True)\n",
    "        # self.classifier = nn.Linear(lstm_hidden_size*2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "        self.init_weights()  \n",
    "\n",
    "    def forward(self, input_ids, input_token_pos, token_type_ids, attention_mask, labels=None, labels_mask=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        origin_sequence_output = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(sequence_output, input_token_pos)]\n",
    "        padded_sequence_output = pad_sequence(origin_sequence_output, batch_first=True)\n",
    "        padded_sequence_output = self.dropout(padded_sequence_output)\n",
    "        # lstm\n",
    "        # lstm_output, _ = self.bilstm(padded_sequence_output)\n",
    "\n",
    "        output = self.classifier(padded_sequence_output)\n",
    "        # lstm\n",
    "        # output = self.classifier(lstm_output)\n",
    "        self.predict = self.crf.decode(output)\n",
    "        if labels is not None:\n",
    "            origin_label = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(labels, input_token_pos)]\n",
    "            padded_label = pad_sequence(origin_label, batch_first=True)\n",
    "            origin_loss_mask = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(labels_mask, input_token_pos)]\n",
    "            padded_loss_mask = pad_sequence(origin_loss_mask, batch_first=True)\n",
    "            loss_mask = padded_loss_mask.gt(0)\n",
    "            loss = self.crf(output, padded_label, loss_mask) * (-1)\n",
    "\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch):\n",
    "    global model\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    token_type_ids = batch['token_type_ids'].to(device)\n",
    "    input_token_pos = batch['input_token_pos'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    labels_mask = batch['labels_mask'].to(device)\n",
    "    \n",
    "    loss = model.forward(input_ids, input_token_pos, token_type_ids, attention_mask, labels, labels_mask)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    global model, optim\n",
    "    epoch = 4\n",
    "\n",
    "    for e in range(epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        model_file = f\"bertNER_lstm_epoch{e+1}\"\n",
    "\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for batch_id, batch in enumerate(loop):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = run(batch)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_id % 500 == 0 and batch_id != 0:\n",
    "                torch.save(model.state_dict(), model_file)\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                    e+1, batch_id, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            loop.set_description(f'Epoch {e+1}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(loader):\n",
    "    global tokenizer\n",
    "    predict_q = []\n",
    "    predict_r = []\n",
    "    predict_q_id = []\n",
    "    predict_r_id = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        input_token_pos = batch['input_token_pos'].to(device)\n",
    "\n",
    "        model.forward(input_ids=input_ids,  input_token_pos=input_token_pos, attention_mask=attention_mask, token_type_ids=token_type_ids)      \n",
    "        prdict_label = model.predict\n",
    "        origin_ids = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(input_ids, input_token_pos)]\n",
    "        origin_type = [layer[starts.nonzero().squeeze(1)] for layer, starts in zip(token_type_ids, input_token_pos)]\n",
    "        for i in range(len(origin_ids)):\n",
    "            for l_id in range(len(origin_ids[i])):\n",
    "                if prdict_label[i][l_id] == 1:\n",
    "                    if origin_type[i][l_id] == 0 and origin_ids[i][l_id] != 100:\n",
    "                        predict_q_id.append(origin_ids[i][l_id])\n",
    "                    elif origin_ids[i][l_id] != 100:\n",
    "                        predict_r_id.append(origin_ids[i][l_id])\n",
    "            p_q = tokenizer.decode(predict_q_id)\n",
    "            p_r = tokenizer.decode(predict_r_id)\n",
    "            predict_q.append(p_q)\n",
    "            predict_r.append(p_r)\n",
    "            predict_q_id = []\n",
    "            predict_r_id = []\n",
    "    \n",
    "    return predict_q, predict_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(\"data_relabel.pkl\", 'rb') as file:\n",
    "        df_train = pickle.load(file)\n",
    "        \n",
    "    train_q = df_train[\"q\"].tolist()\n",
    "    train_r = df_train[\"r\"].tolist()\n",
    "    train_q_label_idx = df_train[\"q_label\"].tolist()\n",
    "    train_r_label_idx = df_train[\"r_label\"].tolist()\n",
    "    train_q_reidx = df_train[\"q_reidx\"].tolist()\n",
    "    train_r_reidx = df_train[\"r_reidx\"].tolist()\n",
    "    \n",
    "    tokenizer = BertTokenizerFast.from_pretrained('dslim/bert-base-NER')\n",
    "    train_encoding = tokenizer(train_q, train_r, truncation=True, padding=True)\n",
    "    train_encoding[\"labels\"],  train_encoding[\"labels_mask\"]= add_label_token(train_encoding, train_q_label_idx, train_r_label_idx, train_q_reidx, train_r_reidx)\n",
    "    train_encoding[\"input_token_pos\"] = get_input_pos(train_encoding)\n",
    "    \n",
    "    batch_size = 4\n",
    "    lstm_hidden_size = 128\n",
    "    lstm_dropout_prob = 0\n",
    "    num_labels = 2\n",
    "\n",
    "    train_dataset = MyDataset(train_encoding)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    model = Mymodel.from_pretrained('dslim/bert-base-NER', lstm_hidden_size, lstm_dropout_prob, num_labels, ignore_mismatched_sizes=True).to(device)\n",
    "    optim = AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_postprocess(df_test, df_answer):\n",
    "    assert len(df_test) == len(df_answer), 'length not match'\n",
    "    \n",
    "    df_answer['q'] = df_answer['q'].apply(lambda x: x.replace(' ##', ''))\n",
    "    df_answer['r'] = df_answer['r'].apply(lambda x: x.replace(' ##', ''))\n",
    "    df_answer['q'] = df_answer['q'].apply(lambda x: x.replace('##', ''))\n",
    "    df_answer['r'] = df_answer['r'].apply(lambda x: x.replace('##', ''))\n",
    "\n",
    "    for idx, row in df_answer.iterrows():\n",
    "        if len(row['q']) == 0:\n",
    "            df_answer.loc[idx, 'q'] = df_test.loc[idx, 'q']\n",
    "        if len(row['r']) == 0:\n",
    "            df_answer.loc[idx, 'r'] = df_test.loc[idx, 'r']\n",
    "    \n",
    "    df_answer[['q', 'r']] = df_answer[['q', 'r']].apply(lambda x: x.str.strip('\\\"'))\n",
    "    df_answer[['q', 'r']] = df_answer[['q', 'r']].apply(lambda x: '\"' + x + '\"')\n",
    "    return df_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit():\n",
    "    df_test = pd.read_csv(\"Batch_answers - test_data(no_label).csv\", encoding = \"utf-8\")\n",
    "    df_test[['q','r']] = df_test[['q','r']].apply(lambda x: x.str.strip('\\\"'))\n",
    "    \n",
    "    test_q = df_test[\"q\"].tolist()\n",
    "    test_r = df_test[\"r\"].tolist()\n",
    "\n",
    "    test_encoding = tokenizer(test_q, test_r, truncation=True, padding=True)\n",
    "    test_encoding[\"input_token_pos\"] = get_input_pos(test_encoding)\n",
    "\n",
    "    test_dataset = MyDataset(test_encoding)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    predict_q, predict_r = pred(test_loader)\n",
    "    df_answer = pd.DataFrame()\n",
    "    df_answer['id'] = df_test['id']\n",
    "    df_answer['q'] = predict_q\n",
    "    df_answer['r'] = predict_r\n",
    "\n",
    "    df_answer = data_postprocess(df_test, df_answer)\n",
    "    df_answer.to_csv('submission_ner.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('AICUP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee0ab1a7bcbdb3aa79eaeb76be23d6ec56290d1a7b450ff99080b97b1bb565c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
