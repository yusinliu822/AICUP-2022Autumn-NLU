{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne_NAWIW17BV"
   },
   "source": [
    "## import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjcgsM7vwcc9",
    "outputId": "a4165ad5-a9a6-4371-860f-86b6a84122c4",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-O0ShLN5PY5"
   },
   "source": [
    "## Data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocess(df, has_answer=True):\n",
    "    if 'Unnamed: 6' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 6'])\n",
    "    if 'total no.: 7987' in df.columns:\n",
    "        df = df.drop(columns=['total no.: 7987'])\n",
    "    \n",
    "    # remove quotation marks\n",
    "    df[['q','r']] = df[['q','r']].apply(lambda x: x.str.strip('\\\"'))\n",
    "    if has_answer:\n",
    "        df[[\"q'\",\"r'\"]] = df[[\"q'\",\"r'\"]].apply(lambda x: x.str.strip('\\\"'))\n",
    "\n",
    "    # concatenate s to r\n",
    "    df['r'] = df['s'] + ':' + df['r']\n",
    "    if 's' in df.columns:\n",
    "        df = df.drop(columns=['s'])\n",
    "    \n",
    "    if has_answer:\n",
    "        # check if q', r' is a substring of q, r\n",
    "        df['sub_q_true'] = [1 if x in y else 0 for x,y in zip(df[\"q'\"],df[\"q\"])]\n",
    "        df['sub_r_true'] = [1 if x in y else 0 for x,y in zip(df[\"r'\"],df[\"r\"])]\n",
    "        df['sub_both'] = df['sub_q_true']*df['sub_r_true']\n",
    "\n",
    "        # extract rows with q is a substring of q' and r is a substring of r'\n",
    "        df = df.loc[df['sub_both'] == 1]\n",
    "\n",
    "        if 'sub_both' in df.columns:\n",
    "            df = df.drop(columns=['sub_q_true', 'sub_r_true', 'sub_both'])\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        df['q_start'] = df.apply(lambda x: x['q'].find(x[\"q'\"]), axis=1)\n",
    "        df['q_end'] = df['q_start'] + df[\"q'\"].str.len() - 1\n",
    "        df['r_start'] = df.apply(lambda x: x['r'].find(x[\"r'\"]), axis=1)\n",
    "        df['r_end'] = df['r_start'] + df[\"r'\"].str.len() - 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcQn-_Rq5yfT"
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def extract_answer(df):\n",
    "    answer = df[['q_start', 'q_end', 'r_start', 'r_end']].to_dict('records')\n",
    "    return answer\n",
    "\n",
    "def add_token_positions(encodings, answers) -> None:\n",
    "    q_start, r_start, q_end, r_end = [],[],[],[]\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        q_start.append(encodings.char_to_token(i, answers[i]['q_start'], 0))\n",
    "        r_start.append(encodings.char_to_token(i, answers[i]['r_start'], 1))\n",
    "        q_end.append(encodings.char_to_token(i, answers[i]['q_end'], 0))\n",
    "        r_end.append(encodings.char_to_token(i, answers[i]['r_end'], 1))\n",
    "\n",
    "        if q_start[-1] is None:\n",
    "            q_start[-1] = 0\n",
    "            q_end[-1] = 0\n",
    "            # continue\n",
    "\n",
    "        if r_start[-1] is None:\n",
    "            r_start[-1] = 0\n",
    "            r_end[-1] = 0\n",
    "            # continue\n",
    "\n",
    "        shift = 1\n",
    "        while q_end[-1] is None:\n",
    "            q_end[-1] = encodings.char_to_token(i, answers[i]['q_end'] - shift)\n",
    "            shift += 1\n",
    "        shift = 1\n",
    "        while r_end[-1] is None:\n",
    "            r_end[-1] = encodings.char_to_token(i, answers[i]['r_end'] - shift)\n",
    "            shift += 1\n",
    "    encodings.update({'q_start':q_start, 'r_start':r_start,\t'q_end':q_end, 'r_end':r_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, df, has_answer=True):\n",
    "    q_list = df['q'].tolist()\n",
    "    r_list = df['r'].tolist()\n",
    "    # qr_encodings = tokenizer(q_list, r_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    qr_encodings = tokenizer(q_list, r_list, padding=True, truncation=True, return_offsets_mapping=True) # for predict_v2\n",
    "    \n",
    "    if has_answer:\n",
    "        answer = extract_answer(df)\n",
    "        add_token_positions(qr_encodings, answer)\n",
    "        \n",
    "    return qr_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGfziDMh6iOP"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awivv8SdNm_d",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class qrDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anB_LCU16q-C"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwZdH-I4PoAf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "from transformers import RobertaModel\n",
    "\n",
    "MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "class qrModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(qrModel, self).__init__()\n",
    "\n",
    "        # self.bert = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "        self.roberta = RobertaModel.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        self.fc = nn.Linear(768, 4)\n",
    "        \n",
    "    # def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        # output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        \n",
    "        logits = output[0]\n",
    "        out = self.fc(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = qrModel().to(device) # Put model on device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEUJSg7E6wWF"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eJKdcP3tdU5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(epoch_idx, valid_loader):\n",
    "    model.eval()\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_batch = int(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(valid_loader, leave=True)\n",
    "        for batch_idx, batch in enumerate(loop):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # token_type_ids = batch['token_type_ids'].to(device)\n",
    "            q_start = batch['q_start'].to(device)\n",
    "            r_start = batch['r_start'].to(device)\n",
    "            q_end = batch['q_end'].to(device)\n",
    "            r_end = batch['r_end'].to(device)\n",
    "\n",
    "            # model output\n",
    "            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "            q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
    "            r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
    "            q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
    "            r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "            q_start_loss = loss_fct(q_start_logits, q_start)\n",
    "            r_start_loss = loss_fct(r_start_logits, r_start)\n",
    "            q_end_loss = loss_fct(q_end_logits, q_end)\n",
    "            r_end_loss = loss_fct(r_end_logits, r_end)\n",
    "\n",
    "            loss = q_start_loss + r_start_loss + q_end_loss + r_end_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "            running_batch += 1\n",
    "            if batch_idx % 60 == 0 and batch_idx != 0:\n",
    "                print('Validation Epoch {} Batch {}/{} Loss {:.4f}'\n",
    "                  .format(epoch_idx + 1, batch_idx + 1, len(loop), running_loss / running_batch))\n",
    "                running_loss = 0.0\n",
    "                running_batch = int(0)\n",
    "    return total_loss / len(valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RIFNMy0WHvk",
    "outputId": "9c97ba42-327e-4b67-9fa6-f5ab22a745d9",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_epoch(epoch_idx, train_loader, optim, scheduler):\n",
    "    \n",
    "    model.train()\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_batach = int(0)\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch_idx, batch in enumerate(loop):\n",
    "        # reset\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "        q_start = batch['q_start'].to(device)\n",
    "        r_start = batch['r_start'].to(device)\n",
    "        q_end = batch['q_end'].to(device)\n",
    "        r_end = batch['r_end'].to(device)\n",
    "\n",
    "        # model output\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
    "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
    "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
    "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        q_start_loss = loss_fct(q_start_logits, q_start)\n",
    "        r_start_loss = loss_fct(r_start_logits, r_start)\n",
    "        q_end_loss = loss_fct(q_end_logits, q_end)\n",
    "        r_end_loss = loss_fct(r_end_logits, r_end)\n",
    "\n",
    "        loss = q_start_loss + r_start_loss + q_end_loss + r_end_loss\n",
    "\n",
    "        # calculate loss\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        running_batach += 1\n",
    "        if batch_idx % 300 == 0 and batch_idx != 0:\n",
    "            print('Epoch {} Batch {}/{} Loss {:.4f}'\n",
    "                  .format(epoch_idx + 1, batch_idx + 1, len(loop), running_loss / running_batach))\n",
    "            running_loss = 0.0\n",
    "            running_batach = int(0)\n",
    "\n",
    "        loop.set_description('Epoch {}'.format(epoch_idx + 1))\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHM3KS8wNI46",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = qrModel().to(device)\n",
    "# model_loss = \"6.9282\"\n",
    "# postfix = \"rosqsr\"\n",
    "# model.load_state_dict(torch.load('../model/simpleQA_model_loss_{}_{}'.format(model_loss, postfix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xi-K6PR7s0_"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjSjODbl312a",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def predict(tokenizer, test_loader):\n",
    "    predict_pos = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    q_sub_output, r_sub_output = [],[]\n",
    "\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        # model output\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
    "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
    "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
    "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        q_start_prdict = torch.argmax(q_start_logits, 1).cpu().numpy()\n",
    "        r_start_prdict = torch.argmax(r_start_logits, 1).cpu().numpy()\n",
    "        q_end_prdict = torch.argmax(q_end_logits, 1).cpu().numpy()\n",
    "        r_end_prdict = torch.argmax(r_end_logits, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            predict_pos.append((q_start_prdict[i].item(), r_start_prdict[i].item(), q_end_prdict[i].item(), r_end_prdict[i].item()))\n",
    "\n",
    "            q_sub = tokenizer.decode(input_ids[i][q_start_prdict[i]:q_end_prdict[i]+1])\n",
    "            r_sub = tokenizer.decode(input_ids[i][r_start_prdict[i]:r_end_prdict[i]+1])\n",
    "            \n",
    "            q_sub_output.append(q_sub)\n",
    "            r_sub_output.append(r_sub)\n",
    "    \n",
    "    return q_sub_output, r_sub_output, predict_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIXjyoo3Sgb0",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_output_post_fn(q_sub_output, r_sub_output):\n",
    "    q_sub, r_sub = [], []\n",
    "    assert len(q_sub_output) == len(r_sub_output), 'length not match'\n",
    "    for i in range(len(q_sub_output)):\n",
    "\n",
    "        q_sub_pred = q_sub_output[i].split()\n",
    "        r_sub_pred = r_sub_output[i].split()\n",
    "\n",
    "        if q_sub_pred is None:\n",
    "            q_sub_pred = []\n",
    "        # q_sub_error_index = q_sub_pred.index('[SEP]') if '[SEP]' in q_sub_pred else -1\n",
    "        q_sub_error_index = q_sub_pred.index('</s>') if '</s>' in q_sub_pred else -1\n",
    "\n",
    "        if q_sub_error_index != -1:\n",
    "            q_sub_pred = q_sub_pred[:q_sub_error_index]\n",
    "\n",
    "        temp = r_sub_pred.copy()\n",
    "        if r_sub_pred is None:\n",
    "            r_sub_pred = []\n",
    "        else:\n",
    "            for j in range(len(temp)):\n",
    "                if temp[j] == '[SEP]':\n",
    "                    r_sub_pred.remove('[SEP]')\n",
    "                if temp[j] == '[PAD]':\n",
    "                    r_sub_pred.remove('[PAD]')\n",
    "                if temp[j] == '</s>':\n",
    "                    r_sub_pred.remove('</s>')\n",
    "                if temp[j] == '<pad>':\n",
    "                    r_sub_pred.remove('<pad>')\n",
    "\n",
    "        q_sub.append(' '.join(q_sub_pred))\n",
    "        r_sub.append(' '.join(r_sub_pred))\n",
    "\n",
    "    return q_sub, r_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_output(tokenizer, test_loader):\n",
    "    q_sub_output, r_sub_output, predict_pos = predict(tokenizer, test_loader)\n",
    "    q_sub, r_sub = get_output_post_fn(q_sub_output, r_sub_output)\n",
    "    return q_sub, r_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def predict_v2(test_encodings, test_loader, batch_size):\n",
    "    predict_pos = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
    "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
    "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
    "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        q_start_prdict = torch.argmax(q_start_logits, 1).cpu().numpy()\n",
    "        r_start_prdict = torch.argmax(r_start_logits, 1).cpu().numpy()\n",
    "        q_end_prdict = torch.argmax(q_end_logits, 1).cpu().numpy()\n",
    "        r_end_prdict = torch.argmax(r_end_logits, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            predict_pos.append((q_start_prdict[i].item(), r_start_prdict[i].item(), q_end_prdict[i].item(), r_end_prdict[i].item()))\n",
    "            if test_encodings.sequence_ids(batch_size * batch_id + i)[predict_pos[-1][2]] != 0:\n",
    "                predict_pos[-1] = (q_start_prdict[i].item(), r_start_prdict[i].item(), test_encodings.sequence_ids(batch_size * batch_id + i).index(1) - 3, r_end_prdict[i].item())\n",
    "            if test_encodings.sequence_ids(batch_size * batch_id + i)[predict_pos[-1][1]] != 1:\n",
    "                predict_pos[-1] = (q_start_prdict[i].item(), test_encodings.sequence_ids(batch_size * batch_id + i).index(1), q_end_prdict[i].item(), r_end_prdict[i].item())\n",
    "    \n",
    "    return predict_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_output_v2(test_encodings, test_offset_mapping, test_loader, test, batch_size):\n",
    "    predict_pos = predict_v2(test_encodings, test_loader, batch_size)\n",
    "    test_q = test['q'].values.tolist()\n",
    "    test_r = test['r'].values.tolist()\n",
    "    q_sub, r_sub = [],[]\n",
    "    for i in range(len(predict_pos)):\n",
    "        q_s = test_offset_mapping[i][predict_pos[i][0]][0]\n",
    "        q_e = test_offset_mapping[i][predict_pos[i][2]][-1]\n",
    "        r_s = test_offset_mapping[i][predict_pos[i][1]][0]\n",
    "        r_e = test_offset_mapping[i][predict_pos[i][3]][-1]\n",
    "        q_pre_sen = test_q[i][q_s:q_e]\n",
    "        r_pre_sen = test_r[i][r_s:r_e]\n",
    "        q_sub.append(q_pre_sen)\n",
    "        r_sub.append(r_pre_sen)\n",
    "    return q_sub, r_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def data_postprocess(df_test, df_answer):\n",
    "    assert len(df_test) == len(df_answer), 'length not match'\n",
    "    \n",
    "    df_answer['q'] = df_answer['q'].apply(lambda x: x.replace(' ##', ''))\n",
    "    df_answer['r'] = df_answer['r'].apply(lambda x: x.replace(' ##', ''))\n",
    "    df_answer['q'] = df_answer['q'].apply(lambda x: x.replace('##', ''))\n",
    "    df_answer['r'] = df_answer['r'].apply(lambda x: x.replace('##', ''))\n",
    "    \n",
    "    for idx, row in df_answer.iterrows():\n",
    "        if len(row['q']) == 0:\n",
    "            df_answer.loc[idx, 'q'] = df_test.loc[idx, 'q']\n",
    "        if len(row['r']) == 0:\n",
    "            df_answer.loc[idx, 'r'] = df_test.loc[idx, 'r']\n",
    "    \n",
    "    df_answer[['q', 'r']] = df_answer[['q', 'r']].apply(lambda x: x.str.strip('\\\"'))\n",
    "    df_answer[['q', 'r']] = df_answer[['q', 'r']].apply(lambda x: '\"' + x + '\"')\n",
    "    return df_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SUC-vq-70Ye"
   },
   "source": [
    "## Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYkzHmd6K9FY",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# grading functions\n",
    "\n",
    "def nltk_token_string(sentence):\n",
    "    # print(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) == 1:\n",
    "            tokens[i] = re.sub(r\"[!\\\"#$%&\\'()*\\+, -.\\/:;<=>?@\\[\\\\\\]^_`{|}~]\", '', tokens[i])\n",
    "    while '' in tokens:\n",
    "        tokens.remove('')\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lcs(X, Y):\n",
    "    X_, Y_ = [], []\n",
    "    \n",
    "    X_ = nltk_token_string(X)\n",
    "    Y_ = nltk_token_string(Y)\n",
    "\n",
    "    m = len(X_)\n",
    "    n = len(Y_)\n",
    " \n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    " \n",
    "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif X_[i-1] == Y_[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "    return L[m][n]\n",
    "\n",
    "\n",
    "def acc(full, sub) -> float:\n",
    "    common = lcs(full, sub)\n",
    "    union = len(nltk_token_string(full)) + len(nltk_token_string(sub)) - common\n",
    "    accuracy = float(common/union) if union != 0 else 1.0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(q_answer, r_answer, q_sub, r_sub):\n",
    "    q_acc, r_acc = [], []\n",
    "    assert len(q_answer) == len(r_answer) == len(q_sub) == len(r_sub), 'length of answer and submission is not same'\n",
    "    \n",
    "    # calculate accuracy\n",
    "    q_acc = [acc(q_answer.iloc[i], q_sub.iloc[i]) for i in range(len(q_answer))]\n",
    "    r_acc = [acc(r_answer.iloc[i], r_sub.iloc[i]) for i in range(len(r_answer))]\n",
    "    q_acc = np.mean(q_acc)\n",
    "    r_acc = np.mean(r_acc)\n",
    "\n",
    "    return q_acc, r_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    LEARNING_RATE = 3e-5\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 8\n",
    "    # Training df file\n",
    "    file = \"../data/Batch_answers - train_data (no-blank).csv\"\n",
    "    df = pd.read_csv(file, encoding = \"utf-8\")\n",
    "    df = data_preprocess(df, has_answer=True)\n",
    "\n",
    "    # train: 80%, valid: 10%, test: 10%\n",
    "    train, valid = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "    # print(len(train), len(valid), len(test))\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_encodings = tokenize(tokenizer, train)\n",
    "    valid_encodings = tokenize(tokenizer, valid)\n",
    "    test_encodings = tokenize(tokenizer, test)\n",
    "    \n",
    "    train_encodings.pop('offset_mapping')\n",
    "    valid_encodings.pop('offset_mapping')\n",
    "    test_offset_mapping = test_encodings['offset_mapping']\n",
    "    test_encodings.pop('offset_mapping')\n",
    "    \n",
    "    train_dataset = qrDataset(train_encodings)\n",
    "    valid_dataset = qrDataset(valid_encodings)\n",
    "    test_dataset = qrDataset(test_encodings)\n",
    "    \n",
    "    # Pack df into dataloader by batch\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    optim = AdamW(model.parameters(), lr=LEARNING_RATE, no_deprecation_warning=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=len(train_loader) * EPOCHS)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch {} / {}'.format(epoch + 1, EPOCHS))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        train_loss = train_epoch(epoch, train_loader, optim, scheduler)\n",
    "        print(\"Total train loss: {}\\n\".format(train_loss))\n",
    "        \n",
    "        eval_loss = evaluate_epoch(epoch, valid_loader)\n",
    "        print(\"Total eval loss: {}\\n\".format(eval_loss))\n",
    "        \n",
    "        torch.save(model.state_dict(), '../model/simpleQA_model_loss_{:.4f}_rosrv2'.format(eval_loss))\n",
    "        \n",
    "    q_sub, r_sub = generate_output_v2(test_encodings, test_offset_mapping, test_loader, test, batch_size=BATCH_SIZE)\n",
    "    df_answer = pd.DataFrame()\n",
    "    df_answer['id'] = test['id']\n",
    "    df_answer['q'] = q_sub\n",
    "    df_answer['r'] = r_sub\n",
    "    \n",
    "    q_acc, r_acc = get_score(test[\"q'\"], test[\"r'\"], df_answer['q'], df_answer['r'])\n",
    "    print('q accuracy: ', q_acc)\n",
    "    print('r accuracy: ', r_acc)\n",
    "    print('total accuracy: ', (q_acc + r_acc)/2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def submit(model_loss, postfix):\n",
    "    BATCH_SIZE = 8\n",
    "    # Training df file\n",
    "    file = \"../data/Batch_answers - test_data(no_label).csv\"\n",
    "    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n",
    "    df = data_preprocess(df, has_answer=False)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    submit_encodings = tokenize(tokenizer, df, has_answer=False)\n",
    "    \n",
    "    submit_offset_mapping = submit_encodings['offset_mapping']\n",
    "    submit_encodings.pop('offset_mapping')\n",
    "    \n",
    "    submit_dataset = qrDataset(submit_encodings)\n",
    "    \n",
    "    # Pack df into dataloader by batch\n",
    "    submit_loader = DataLoader(submit_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # RELOAD MODEL\n",
    "    \n",
    "    # q_sub, r_sub = generate_output(tokenizer, submit_loader)\n",
    "    q_sub, r_sub = generate_output_v2(submit_encodings, submit_offset_mapping, submit_loader, df , batch_size=BATCH_SIZE)\n",
    "    df_answer = pd.DataFrame()\n",
    "    df_answer['id'] = df['id']\n",
    "    df_answer['q'] = q_sub\n",
    "    df_answer['r'] = r_sub\n",
    "\n",
    "    df_answer = data_postprocess(df, df_answer)\n",
    "    df_answer.to_csv('../data/submission_simpleQA_{}_{}.csv'.format(postfix, model_loss), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model_loss = \"6.6521\"\n",
    "# postfix = \"rosr\"\n",
    "# submit(model_loss, postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# loss_list = [\"7.0940\"]\n",
    "# model = qrModel().to(device)\n",
    "# for (i, model_loss) in enumerate(loss_list):\n",
    "#     # load model\n",
    "#     postfix = \"rosqsr\"\n",
    "#     model.load_state_dict(torch.load('../model/simpleQA_model_loss_{}_{}'.format(model_loss, postfix)))\n",
    "#     print(\"Epoch: \", i+1,\" model loss: \", model_loss)\n",
    "#     main()\n",
    "#     submit(model_loss, postfix)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
