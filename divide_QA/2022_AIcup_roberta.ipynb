{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne_NAWIW17BV"
      },
      "source": [
        "## import package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjcgsM7vwcc9",
        "outputId": "a4165ad5-a9a6-4371-860f-86b6a84122c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import torch, pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "# nltk.download('punkt')\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(123)\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SYk_hc2dpscj"
      },
      "outputs": [],
      "source": [
        "# Training data file\n",
        "# directory=\"/content/drive/MyDrive/Colab Notebooks/AIcup2022/\"\n",
        "\n",
        "\n",
        "file=\"/home/shuxian109504502/AICUP/data/data_fix_label_to_sen.pkl\"\n",
        "with open(file, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "# df=pd.read_csv(file, encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "      <th>s</th>\n",
              "      <th>q'</th>\n",
              "      <th>r'</th>\n",
              "      <th>q_label</th>\n",
              "      <th>r_label</th>\n",
              "      <th>q_reidx</th>\n",
              "      <th>r_reidx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>It can go both ways . We all doubt . It is wha...</td>\n",
              "      <td>True .</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>It can go both ways . We all doubt . It is wha...</td>\n",
              "      <td>True .</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 5)</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>It can go both ways . We all doubt . It is wha...</td>\n",
              "      <td>True .</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>can go both ways . We all doubt . It is what y...</td>\n",
              "      <td>True</td>\n",
              "      <td>(3, 74)</td>\n",
              "      <td>(0, 3)</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>It can go both ways . We all doubt . It is wha...</td>\n",
              "      <td>True .</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>It can go both ways . We all doubt . It is wha...</td>\n",
              "      <td>True</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 3)</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 5)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>once again , you seem to support the killing o...</td>\n",
              "      <td>based on the idea that people are dispensible ...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>seem to support the killing of certain people</td>\n",
              "      <td>based on the idea that people are dispensible ...</td>\n",
              "      <td>(17, 61)</td>\n",
              "      <td>(0, 92)</td>\n",
              "      <td>(0, 81)</td>\n",
              "      <td>(0, 337)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>once again , you seem to support the killing o...</td>\n",
              "      <td>based on the idea that people are dispensible ...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>you seem to support the killing of certain peo...</td>\n",
              "      <td>based on the idea that people are dispensible</td>\n",
              "      <td>(13, 81)</td>\n",
              "      <td>(0, 44)</td>\n",
              "      <td>(0, 81)</td>\n",
              "      <td>(0, 337)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36867</th>\n",
              "      <td>10001</td>\n",
              "      <td>good thing this argument has never been done !...</td>\n",
              "      <td>And teen sex does n't , by the very nature of ...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>You are much better off making theft legal and...</td>\n",
              "      <td>And teen sex does n't , by the very nature of ...</td>\n",
              "      <td>(111, 227)</td>\n",
              "      <td>(0, 57)</td>\n",
              "      <td>(0, 227)</td>\n",
              "      <td>(0, 200)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36868</th>\n",
              "      <td>10002</td>\n",
              "      <td>I know one thing , anything that happens , pol...</td>\n",
              "      <td>Was n't sinjin crowing about his plans to take...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>I know one thing , anything that happens , pol...</td>\n",
              "      <td>Was n't sinjin crowing about his plans to take...</td>\n",
              "      <td>(0, 108)</td>\n",
              "      <td>(0, 76)</td>\n",
              "      <td>(0, 643)</td>\n",
              "      <td>(0, 260)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36869</th>\n",
              "      <td>10002</td>\n",
              "      <td>I know one thing , anything that happens , pol...</td>\n",
              "      <td>Was n't sinjin crowing about his plans to take...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>FBI Arrests Three Men in Terror Plot that Targ...</td>\n",
              "      <td>Was n't sinjin crowing about his plans to take...</td>\n",
              "      <td>(112, 195)</td>\n",
              "      <td>(0, 56)</td>\n",
              "      <td>(0, 643)</td>\n",
              "      <td>(0, 260)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36870</th>\n",
              "      <td>10003</td>\n",
              "      <td>I enjoy Botany more than most things and I hav...</td>\n",
              "      <td>Hi Smallax , welcome to the forum . I did a se...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>I enjoy Botany more than most things and I hav...</td>\n",
              "      <td>Hi Smallax , welcome to the forum . I did a se...</td>\n",
              "      <td>(0, 106)</td>\n",
              "      <td>(0, 119)</td>\n",
              "      <td>(0, 442)</td>\n",
              "      <td>(0, 266)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36871</th>\n",
              "      <td>10003</td>\n",
              "      <td>I enjoy Botany more than most things and I hav...</td>\n",
              "      <td>Hi Smallax , welcome to the forum . I did a se...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>bringing in outside sun light through fiber op...</td>\n",
              "      <td>might give you an idea about costs and concept...</td>\n",
              "      <td>(155, 225)</td>\n",
              "      <td>(86, 168)</td>\n",
              "      <td>(0, 442)</td>\n",
              "      <td>(0, 266)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36872 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                                  q  \\\n",
              "0          8  It can go both ways . We all doubt . It is wha...   \n",
              "1          8  It can go both ways . We all doubt . It is wha...   \n",
              "2          8  It can go both ways . We all doubt . It is wha...   \n",
              "3          9  once again , you seem to support the killing o...   \n",
              "4          9  once again , you seem to support the killing o...   \n",
              "...      ...                                                ...   \n",
              "36867  10001  good thing this argument has never been done !...   \n",
              "36868  10002  I know one thing , anything that happens , pol...   \n",
              "36869  10002  I know one thing , anything that happens , pol...   \n",
              "36870  10003  I enjoy Botany more than most things and I hav...   \n",
              "36871  10003  I enjoy Botany more than most things and I hav...   \n",
              "\n",
              "                                                       r         s  \\\n",
              "0                                                 True .     AGREE   \n",
              "1                                                 True .     AGREE   \n",
              "2                                                 True .     AGREE   \n",
              "3      based on the idea that people are dispensible ...     AGREE   \n",
              "4      based on the idea that people are dispensible ...     AGREE   \n",
              "...                                                  ...       ...   \n",
              "36867  And teen sex does n't , by the very nature of ...  DISAGREE   \n",
              "36868  Was n't sinjin crowing about his plans to take...  DISAGREE   \n",
              "36869  Was n't sinjin crowing about his plans to take...  DISAGREE   \n",
              "36870  Hi Smallax , welcome to the forum . I did a se...     AGREE   \n",
              "36871  Hi Smallax , welcome to the forum . I did a se...     AGREE   \n",
              "\n",
              "                                                      q'  \\\n",
              "0      It can go both ways . We all doubt . It is wha...   \n",
              "1      can go both ways . We all doubt . It is what y...   \n",
              "2      It can go both ways . We all doubt . It is wha...   \n",
              "3          seem to support the killing of certain people   \n",
              "4      you seem to support the killing of certain peo...   \n",
              "...                                                  ...   \n",
              "36867  You are much better off making theft legal and...   \n",
              "36868  I know one thing , anything that happens , pol...   \n",
              "36869  FBI Arrests Three Men in Terror Plot that Targ...   \n",
              "36870  I enjoy Botany more than most things and I hav...   \n",
              "36871  bringing in outside sun light through fiber op...   \n",
              "\n",
              "                                                      r'     q_label  \\\n",
              "0                                                 True .     (0, 76)   \n",
              "1                                                   True     (3, 74)   \n",
              "2                                                   True     (0, 76)   \n",
              "3      based on the idea that people are dispensible ...    (17, 61)   \n",
              "4          based on the idea that people are dispensible    (13, 81)   \n",
              "...                                                  ...         ...   \n",
              "36867  And teen sex does n't , by the very nature of ...  (111, 227)   \n",
              "36868  Was n't sinjin crowing about his plans to take...    (0, 108)   \n",
              "36869  Was n't sinjin crowing about his plans to take...  (112, 195)   \n",
              "36870  Hi Smallax , welcome to the forum . I did a se...    (0, 106)   \n",
              "36871  might give you an idea about costs and concept...  (155, 225)   \n",
              "\n",
              "         r_label   q_reidx   r_reidx  \n",
              "0         (0, 5)   (0, 76)    (0, 5)  \n",
              "1         (0, 3)   (0, 76)    (0, 5)  \n",
              "2         (0, 3)   (0, 76)    (0, 5)  \n",
              "3        (0, 92)   (0, 81)  (0, 337)  \n",
              "4        (0, 44)   (0, 81)  (0, 337)  \n",
              "...          ...       ...       ...  \n",
              "36867    (0, 57)  (0, 227)  (0, 200)  \n",
              "36868    (0, 76)  (0, 643)  (0, 260)  \n",
              "36869    (0, 56)  (0, 643)  (0, 260)  \n",
              "36870   (0, 119)  (0, 442)  (0, 266)  \n",
              "36871  (86, 168)  (0, 442)  (0, 266)  \n",
              "\n",
              "[36872 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['sub_q_true'] = [1 if x != None else -1 for x in data[\"q_label\"]]\n",
        "data['sub_r_true'] = [1 if x != None else -1 for x in data[\"r_label\"]]\n",
        "data['sub_both'] = data['sub_q_true'] * data['sub_r_true']\n",
        "data.drop(index= data[data['sub_both'] == -1].index, inplace=True)\n",
        "data.drop(columns=['sub_q_true', 'sub_r_true', 'sub_both'], inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-O0ShLN5PY5"
      },
      "source": [
        "## Data process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sra75dJjjZwC"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = data[:int(len(data)*0.9)].copy()\n",
        "valid = data[int(len(data)*0.9):].copy()\n",
        "del data\n",
        "# train, valid = train_test_split(data, test_size=1/9, shuffle=False)\n",
        "# valid, test = train_test_split(valid, test_size=0.5)\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "valid.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train[\"s+r\"] = train[\"s\"] + \": \" + train[\"r\"]\n",
        "valid[\"s+r\"] = valid[\"s\"] + \": \" + valid[\"r\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcQn-_Rq5yfT"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kJNMGgErziHa"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "# MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TozH0A1pm2bv"
      },
      "outputs": [],
      "source": [
        "train_data_q = train['q'].tolist()\n",
        "valid_data_q = valid['q'].tolist()\n",
        "# test_data_q = test['q'].tolist()\n",
        "\n",
        "train_data_r = train['s+r'].tolist()\n",
        "valid_data_r = valid['s+r'].tolist()\n",
        "# test_data_r = test['r'].tolist()\n",
        "\n",
        "train_s = train['s'].tolist()\n",
        "valid_s = valid['s'].tolist()\n",
        "\n",
        "train_q_label = train['q_label'].tolist()\n",
        "valid_q_label = valid['q_label'].tolist()\n",
        "\n",
        "train_r_label = train['r_label'].tolist()\n",
        "valid_r_label = valid['r_label'].tolist()\n",
        "\n",
        "train_q_reidx = train['q_reidx'].tolist()\n",
        "valid_q_reidx = valid['q_reidx'].tolist()\n",
        "# test_q_reidx = test['q_reidx'].tolist()\n",
        "\n",
        "train_r_reidx = train['r_reidx'].tolist()\n",
        "valid_r_reidx = valid['r_reidx'].tolist()\n",
        "# test_r_reidx = test['r_reidx'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a4-Xb_k1kfyv"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_data_q, train_data_r, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(valid_data_q, valid_data_r, truncation=True, padding=True, max_length=512, return_offsets_mapping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGfziDMh6iOP"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_token_positions(encodings, q_label, r_label, q_reidx, r_reidx, s_data=None):\n",
        "    if s_data is not None:\n",
        "        for idx, s in enumerate(s_data):\n",
        "            if s == \"AGREE\":\n",
        "                r_label[idx] = (r_label[idx][0] + 7, r_label[idx][1] + 7) if r_label[idx] != None else None\n",
        "            elif s == \"DISAGREE\":\n",
        "                r_label[idx] = (r_label[idx][0] + 10, r_label[idx][1] + 10) if r_label[idx] != None else None\n",
        "    q_starts, r_starts, q_ends, r_ends = [], [], [], []\n",
        "    for idx, (q_l, q_r, r_l, r_r) in enumerate(zip(q_label, q_reidx, r_label, r_reidx)):\n",
        "        # q_start, q_end, r_start, r_end = 0, 0, 0, 0\n",
        "\n",
        "        if q_l == None or r_l == None:\n",
        "            q_starts.append(0)\n",
        "            q_ends.append(0)\n",
        "            r_starts.append(0)\n",
        "            r_ends.append(0)\n",
        "            continue\n",
        "\n",
        "        q_s = encodings.char_to_token(idx, q_l[0]-q_r[0], 0)\n",
        "        q_e = encodings.char_to_token(idx, q_l[1]-q_r[0], 0)\n",
        "\n",
        "        r_s = encodings.char_to_token(idx, r_l[0]-r_r[0], 1)    #2\n",
        "        r_e = encodings.char_to_token(idx, r_l[1]-r_r[0], 1)\n",
        "\n",
        "        if q_s == None and q_e == None or r_s == None and r_e == None:\n",
        "            q_starts.append(0)\n",
        "            q_ends.append(0)\n",
        "            r_starts.append(0)\n",
        "            r_ends.append(0)\n",
        "            continue\n",
        "\n",
        "        shift = 1\n",
        "        while q_s is None:\n",
        "            q_s = encodings.char_to_token(idx, q_l[0]-q_r[0] + shift, 0)\n",
        "            shift += 1\n",
        "        shift = 1\n",
        "        while r_s is None:\n",
        "            r_s = encodings.char_to_token(idx, r_l[0]-r_r[0] + shift, 1)    #2\n",
        "            shift += 1\n",
        "\n",
        "        shift = 1\n",
        "        while q_e is None:\n",
        "            q_e = encodings.char_to_token(idx, q_l[1]-q_r[0] - shift, 0)\n",
        "            shift += 1\n",
        "        shift = 1\n",
        "        while r_e is None:\n",
        "            r_e = encodings.char_to_token(idx, r_l[1]-r_r[0] - shift, 1)    #2\n",
        "            shift += 1\n",
        "            \n",
        "        if q_s == None or q_e == None or r_s == None or r_e == None:\n",
        "            print(idx, q_s, q_e, r_s, r_e)\n",
        "        q_starts.append(q_s)\n",
        "        q_ends.append(q_e)\n",
        "        r_starts.append(r_s)\n",
        "        r_ends.append(r_e)\n",
        "        # print(idx, q_s,q_e,r_s,r_e)\n",
        "\n",
        "    encodings.update({'q_start': q_starts, 'q_end': q_ends, 'r_start': r_starts, 'r_end': r_ends})\n",
        "    return r_label, r_reidx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "A1xqKVrBwSHx"
      },
      "outputs": [],
      "source": [
        "# Convert char_based_id to token_based_id\n",
        "# Find the corossponding token id after input being tokenized\n",
        "train_r_label, train_r_reidx =  add_token_positions(train_encodings, train_q_label, train_r_label, train_q_reidx, train_r_reidx, train_s)\n",
        "valid_r_label, valid_r_reidx =  add_token_positions(val_encodings, valid_q_label, valid_r_label, valid_q_reidx, valid_r_reidx, valid_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Awivv8SdNm_d"
      },
      "outputs": [],
      "source": [
        "class qrDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # item = {}\n",
        "        # for key, val in self.encodings.items():\n",
        "        #     if key != 'offset_mapping':\n",
        "        #         item[key] = torch.tensor(val[idx])\n",
        "        # return item\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yIjnUh9qOEjD"
      },
      "outputs": [],
      "source": [
        "val_mappping = val_encodings['offset_mapping']\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = qrDataset(train_encodings)\n",
        "val_dataset = qrDataset(val_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZhJSrR7xTjv",
        "outputId": "f99c03c1-d4b9-4f3a-90b5-9b311eb9fc6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dict_keys(['input_ids', 'attention_mask', 'q_start', 'q_end', 'r_start', 'r_end']),\n",
              " dict_keys(['input_ids', 'attention_mask', 'q_start', 'q_end', 'r_start', 'r_end']))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.encodings.keys(), val_dataset.encodings.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anB_LCU16q-C"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rwZdH-I4PoAf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "class myModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(myModel, self).__init__()\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.fc = nn.Linear(768, 4)\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None):   \n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        # output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
        "        output_logits = self.fc(output[0])\n",
        "        return output_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUJSg7E6wWF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JIwySrYN9EOp"
      },
      "outputs": [],
      "source": [
        "# Pack data into dataloader by batch\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIZ50YA8OfgB",
        "outputId": "bf24afe9-4b0c-4421-b8cd-69d584db4799"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Set GPU / CPU\n",
        "# Put model on device\n",
        "model = myModel().to(device)\n",
        "training_epoch = 3\n",
        "loss_fct = CrossEntropyLoss()\n",
        "\n",
        "# params = list(model.named_parameters())\n",
        "# no_decay = ['bias,','LayerNorm']\n",
        "# other = ['fc']\n",
        "# no_main = no_decay + other\n",
        "\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params':[p for n,p in params if not any(nd in n for nd in no_main)],'weight_decay':1e-2,'lr':1e-5},\n",
        "#     {'params':[p for n,p in params if not any(nd in n for nd in other) and any(nd in n for nd in no_decay) ],'weight_decay':0,'lr':1e-5},\n",
        "#     {'params':[p for n,p in params if any(nd in n for nd in other) and any(nd in n for nd in no_decay) ],'weight_decay':0,'lr':1e-2},\n",
        "#     {'params':[p for n,p in params if any(nd in n for nd in other) and not any(nd in n for nd in no_decay) ],'weight_decay':1e-2,'lr':1e-2},\n",
        "# ]\n",
        "total_steps = len(train_loader) * training_epoch\n",
        "optim = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# optim = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SUC-vq-70Ye"
      },
      "source": [
        "### Grading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vIXjyoo3Sgb0"
      },
      "outputs": [],
      "source": [
        "def get_output_post_fn(test, q_sub_output, r_sub_output):\n",
        "    q_sub, r_sub = [], []\n",
        "    for i in range(len(test)):\n",
        "\n",
        "        q_sub_pred = q_sub_output[i].split()\n",
        "        r_sub_pred = r_sub_output[i].split()\n",
        "\n",
        "        if q_sub_pred is None:\n",
        "            q_sub_pred = []\n",
        "        q_sub_error_index = q_sub_pred.index('</s>') if '</s>' in q_sub_pred else -1\n",
        "\n",
        "        if q_sub_error_index != -1:\n",
        "            q_sub_pred = q_sub_pred[:q_sub_error_index]\n",
        "\n",
        "        temp = r_sub_pred.copy()\n",
        "        if r_sub_pred is None:\n",
        "            r_sub_pred = []\n",
        "        else:\n",
        "            for j in range(len(temp)):\n",
        "                if temp[j] == '</s>':\n",
        "                    r_sub_pred.remove('</s>')\n",
        "                if temp[j] == '<pad>':\n",
        "                    r_sub_pred.remove('<pad>')\n",
        "\n",
        "        q_sub.append(' '.join(q_sub_pred))\n",
        "        r_sub.append(' '.join(r_sub_pred))\n",
        "\n",
        "        if q_sub[-1] == \"<s>\":\n",
        "            q_sub[-1] = test[\"q\"][len(q_sub)-1]\n",
        "        if r_sub[-1] == \"<s>\":\n",
        "            r_sub[-1] = test[\"r\"][len(r_sub)-1]\n",
        "\n",
        "    return q_sub, r_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "S6QjZ3G5fL90"
      },
      "outputs": [],
      "source": [
        "def nltk_token_string(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    for i in range(len(tokens)):\n",
        "        if len(tokens[i]) == 1:\n",
        "            tokens[i] = re.sub(r\"[!\\\"#$%&\\'()*\\+, -.\\/:;<=>?@\\[\\\\\\]^_`{|}~]\", '', tokens[i])\n",
        "    while '' in tokens:\n",
        "        tokens.remove('')\n",
        "    # tokens = ' '.join(tokens)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xYkzHmd6K9FY"
      },
      "outputs": [],
      "source": [
        "def lcs(X, Y):\n",
        "    X_, Y_ = [], []\n",
        "    X_ = nltk_token_string(X)\n",
        "    Y_ = nltk_token_string(Y)\n",
        "\n",
        "    m = len(X_)\n",
        "    n = len(Y_)\n",
        " \n",
        "    # declaring the array for storing the dp values\n",
        "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
        " \n",
        "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
        "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
        "    and Y[0..j-1]\"\"\"\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            if i == 0 or j == 0 :\n",
        "                L[i][j] = 0\n",
        "            elif X_[i-1] == Y_[j-1]:\n",
        "                L[i][j] = L[i-1][j-1]+1\n",
        "            else:\n",
        "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
        " \n",
        "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
        "    return L[m][n], m, n\n",
        "\n",
        "\n",
        "def acc_(full, sub):\n",
        "    common, m, n = lcs(full, sub)\n",
        "    union = m + n - common\n",
        "    if union == 0:\n",
        "        return 1\n",
        "    accuracy = float(common/union)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_acc(q_true, r_true, q_sub, r_sub):\n",
        "    q_acc_sum = 0\n",
        "    r_acc_sum = 0\n",
        "    test_len = len(q_true)\n",
        "    for i in range(test_len):\n",
        "        q_accuracy = acc_(q_true[i], q_sub[i])\n",
        "        r_accuracy = acc_(r_true[i], r_sub[i])\n",
        "\n",
        "        q_acc_sum += q_accuracy\n",
        "        r_acc_sum += r_accuracy\n",
        "\n",
        "    print(\"q accuracy: \", q_acc_sum/test_len)\n",
        "    print(\"r accuracy: \", r_acc_sum/test_len)\n",
        "    return q_acc_sum/test_len, r_acc_sum/test_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6eJKdcP3tdU5"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid_loader, valid_q, valid_r):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    predict_pos, q_sub_output, r_sub_output = [], [], []\n",
        "    q_true_output, r_true_output = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loop = tqdm(valid_loader, leave=True, ncols=75)\n",
        "        for batch_id, batch in enumerate(loop):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            # token_type_ids = batch['token_type_ids'].to(device)\n",
        "            q_start = batch['q_start'].to(device)\n",
        "            r_start = batch['r_start'].to(device)\n",
        "            q_end = batch['q_end'].to(device)\n",
        "            r_end = batch['r_end'].to(device)\n",
        "\n",
        "            # model output\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "            q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
        "\n",
        "            q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
        "            r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
        "            q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
        "            r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "            q_start_loss = loss_fct(q_start_logits, q_start)\n",
        "            r_start_loss = loss_fct(r_start_logits, r_start)\n",
        "            q_end_loss = loss_fct(q_end_logits, q_end)\n",
        "            r_end_loss = loss_fct(r_end_logits, r_end)\n",
        "\n",
        "            loss = q_start_loss + r_start_loss + q_end_loss + r_end_loss\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            if batch_id % 250 == 0 and batch_id != 0:\n",
        "                print('Validation Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                    batch_id + 1, batch_id, running_loss / 250))\n",
        "                running_loss = 0.0\n",
        "\n",
        "            q_start_prdict = torch.argmax(q_start_logits, 1).cpu().numpy()\n",
        "            r_start_prdict = torch.argmax(r_start_logits, 1).cpu().numpy()\n",
        "            q_end_prdict = torch.argmax(q_end_logits, 1).cpu().numpy()\n",
        "            r_end_prdict = torch.argmax(r_end_logits, 1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(input_ids)):\n",
        "                predict_pos.append((q_start_prdict[i].item(), r_start_prdict[i].item(), q_end_prdict[i].item(), r_end_prdict[i].item()))\n",
        "\n",
        "                # q_sub = tokenizer.decode(input_ids[i][q_start_prdict[i]:q_end_prdict[i]+1])\n",
        "                # r_sub = tokenizer.decode(input_ids[i][r_start_prdict[i]:r_end_prdict[i]+1])\n",
        "                # q_true = tokenizer.decode(input_ids[i][q_start[i]:q_end[i]+1])\n",
        "                # r_true = tokenizer.decode(input_ids[i][r_start[i]:r_end[i]+1])\n",
        "                q_true_s = val_mappping[batch_size * batch_id + i][q_start[i]][0]\n",
        "                q_true_e = val_mappping[batch_size * batch_id + i][q_end[i]][-1]\n",
        "                r_true_s = val_mappping[batch_size * batch_id + i][r_start[i]][0]\n",
        "                r_true_e = val_mappping[batch_size * batch_id + i][r_end[i]][-1]\n",
        "                q_true = valid_q[batch_size * batch_id + i][q_true_s:q_true_e]\n",
        "                r_true = valid_r[batch_size * batch_id + i][r_true_s:r_true_e]\n",
        "\n",
        "                q_s = val_mappping[batch_size * batch_id + i][predict_pos[-1][0]][0]\n",
        "                q_e = val_mappping[batch_size * batch_id + i][predict_pos[-1][2]][-1]\n",
        "                r_s = val_mappping[batch_size * batch_id + i][predict_pos[-1][1]][0]\n",
        "                r_e = val_mappping[batch_size * batch_id + i][predict_pos[-1][3]][-1]\n",
        "                q_sub = valid_q[batch_size * batch_id + i][q_s:q_e]\n",
        "                r_sub = valid_r[batch_size * batch_id + i][r_s:r_e]\n",
        "\n",
        "                q_sub_output.append(q_sub)\n",
        "                r_sub_output.append(r_sub)\n",
        "                q_true_output.append(q_true)\n",
        "                r_true_output.append(r_true)\n",
        "\n",
        "        print(\"evaluate loss: \", total_loss / len(valid_loader))\n",
        "        # q_sub, r_sub = get_output_post_fn(valid, q_sub_output, r_sub_output)\n",
        "    return q_sub_output, r_sub_output, q_true_output, r_true_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RIFNMy0WHvk",
        "outputId": "9c97ba42-327e-4b67-9fa6-f5ab22a745d9"
      },
      "outputs": [],
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(training_epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, leave=True, ncols=75)\n",
        "    for batch_id, batch in enumerate(loop):\n",
        "        # reset\n",
        "        optim.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # token_type_ids = batch['token_type_ids'].to(device)\n",
        "        q_start = batch['q_start'].to(device)\n",
        "        r_start = batch['r_start'].to(device)\n",
        "        q_end = batch['q_end'].to(device)\n",
        "        r_end = batch['r_end'].to(device)\n",
        "\n",
        "\n",
        "        # model output\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
        "\n",
        "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
        "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
        "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
        "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        q_start_loss = loss_fct(q_start_logits, q_start)\n",
        "        r_start_loss = loss_fct(r_start_logits, r_start)\n",
        "        q_end_loss = loss_fct(q_end_logits, q_end)\n",
        "        r_end_loss = loss_fct(r_end_logits, r_end)\n",
        "\n",
        "        loss = q_start_loss + r_start_loss + q_end_loss + r_end_loss\n",
        "\n",
        "        # calculate loss\n",
        "        loss.backward()\n",
        "        running_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        # update parameters\n",
        "        clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        if batch_id % 500 == 0 and batch_id != 0 or batch_id == len(train_loader) - 1:\n",
        "            print('Step {} Batch {} Loss {:.4f}'.format(\n",
        "                batch_id + 1, batch_id, (running_loss / 500) if batch_id != len(train_loader) - 1 or(len(train_loader) % 500) ==0 else running_loss / (len(train_loader) % 500)))\n",
        "            running_loss = 0.0\n",
        "\n",
        "        loop.set_description('Epoch {}'.format(epoch + 1))\n",
        "        loop.set_postfix(loss=total_loss/(batch_id+1))\n",
        "    # evaluate(valid_loader)\n",
        "    q_sub_output, r_sub_output, q_true_output, r_true_output = evaluate(valid_loader, valid_data_q, valid_data_r)\n",
        "    # q_sub, r_sub = get_output_post_fn(valid, q_sub_output, r_sub_output)\n",
        "    acc_q, acc_r = get_acc(q_true_output, r_true_output, q_sub_output, r_sub_output)\n",
        "    acc = (acc_q + acc_r) / 2\n",
        "    print(\"acc:\", acc)\n",
        "    # print(\"before: \", acc, best_acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_model_name = str(best_acc)\n",
        "        torch.save(model.state_dict(), best_model_name)\n",
        "        print(\"save model----acc: \", best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oHM3KS8wNI46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = myModel().to(device)\n",
        "model.load_state_dict(torch.load(best_model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xi-K6PR7s0_"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vjSjODbl312a"
      },
      "outputs": [],
      "source": [
        "def predict(test_loader, test_encodings):\n",
        "    predict_pos = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    q_sub_output, r_sub_output = [],[]\n",
        "\n",
        "    loop = tqdm(test_loader, leave=True)\n",
        "    for batch_id, batch in enumerate(loop):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "        # model output\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "        q_start_logits, r_start_logits, q_end_logits, r_end_logits = torch.split(outputs, 1, 2)\n",
        "\n",
        "        q_start_logits = q_start_logits.squeeze(-1).contiguous()\n",
        "        r_start_logits = r_start_logits.squeeze(-1).contiguous()\n",
        "        q_end_logits = q_end_logits.squeeze(-1).contiguous()\n",
        "        r_end_logits = r_end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        q_start_prdict = torch.argmax(q_start_logits, 1).cpu().numpy()\n",
        "        r_start_prdict = torch.argmax(r_start_logits, 1).cpu().numpy()\n",
        "        q_end_prdict = torch.argmax(q_end_logits, 1).cpu().numpy()\n",
        "        r_end_prdict = torch.argmax(r_end_logits, 1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(input_ids)):\n",
        "            predict_pos.append((q_start_prdict[i].item(), r_start_prdict[i].item(), q_end_prdict[i].item(), r_end_prdict[i].item()))\n",
        "\n",
        "            if test_encodings.sequence_ids(batch_size * batch_id + i)[predict_pos[-1][2]] != 0:\n",
        "                predict_pos[-1] = (q_start_prdict[i].item(), r_start_prdict[i].item(), test_encodings.sequence_ids(batch_size * batch_id + i).index(1) - 3, r_end_prdict[i].item())\n",
        "            if test_encodings.sequence_ids(batch_size * batch_id + i)[predict_pos[-1][1]] != 1:\n",
        "                predict_pos[-1] = (q_start_prdict[i].item(), test_encodings.sequence_ids(batch_size * batch_id + i).index(1), q_end_prdict[i].item(), r_end_prdict[i].item())\n",
        "                \n",
        "            # q_sub_output.append(q_sub)\n",
        "            # r_sub_output.append(r_sub)\n",
        "    \n",
        "    return predict_pos  #q_sub_output, r_sub_output, predict_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == \"\":\n",
            "/home/shuxian109504502/anaconda3/envs/AICUP/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "      <th>s</th>\n",
              "      <th>q_reidx</th>\n",
              "      <th>r_reidx</th>\n",
              "      <th>q_sub_idx</th>\n",
              "      <th>r_sub_idx</th>\n",
              "      <th>s+r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2378</th>\n",
              "      <td>5227</td>\n",
              "      <td>Why so many what ?</td>\n",
              "      <td>Different sciences . There are different spins...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>(0, 17)</td>\n",
              "      <td>(0, 106)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>AGREE: Different sciences . There are differen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2379</th>\n",
              "      <td>6136</td>\n",
              "      <td>Why would someone make that up and pass it off...</td>\n",
              "      <td>Well , they did .</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>(0, 65)</td>\n",
              "      <td>(0, 16)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>AGREE: Well , they did .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2380</th>\n",
              "      <td>2271</td>\n",
              "      <td>You once said that you had done a detailed stu...</td>\n",
              "      <td>So doing a detailed study of something require...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 318)</td>\n",
              "      <td>(0, 210)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: So doing a detailed study of somethi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2381</th>\n",
              "      <td>4420</td>\n",
              "      <td>Woodward was a fraud , and I recall that was a...</td>\n",
              "      <td>do you have proof of such a statement ? And wh...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 77)</td>\n",
              "      <td>(0, 166)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: do you have proof of such a statemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2382</th>\n",
              "      <td>4071</td>\n",
              "      <td>Would you accept civil unions even though they...</td>\n",
              "      <td>No ... and you do n't have to either .</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 71)</td>\n",
              "      <td>(0, 37)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: No ... and you do n't have to either .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2383</th>\n",
              "      <td>9499</td>\n",
              "      <td>You are betraying your belief system .</td>\n",
              "      <td>Yep . ( I 'm assuming that by `` belief system...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>(0, 37)</td>\n",
              "      <td>(0, 270)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>AGREE: Yep . ( I 'm assuming that by `` belief...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2384</th>\n",
              "      <td>4611</td>\n",
              "      <td>You are in a loud minority , railing against t...</td>\n",
              "      <td>Being in the minority or in the majority is ir...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 77)</td>\n",
              "      <td>(0, 90)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: Being in the minority or in the majo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2385</th>\n",
              "      <td>9328</td>\n",
              "      <td>You bet your XXX that 'd make me happy .</td>\n",
              "      <td>Well , first , I probably would n't bet my XXX...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 39)</td>\n",
              "      <td>(0, 237)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: Well , first , I probably would n't ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2386</th>\n",
              "      <td>5225</td>\n",
              "      <td>you say `` f * * * the Constitution. ``</td>\n",
              "      <td>and gun nuts say f * * * the children when we ...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 38)</td>\n",
              "      <td>(0, 99)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: and gun nuts say f * * * the childre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2387</th>\n",
              "      <td>68</td>\n",
              "      <td>Your answers were without content or meaning ....</td>\n",
              "      <td>k</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 68)</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: k</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                                  q  \\\n",
              "2378  5227                                 Why so many what ?   \n",
              "2379  6136  Why would someone make that up and pass it off...   \n",
              "2380  2271  You once said that you had done a detailed stu...   \n",
              "2381  4420  Woodward was a fraud , and I recall that was a...   \n",
              "2382  4071  Would you accept civil unions even though they...   \n",
              "2383  9499             You are betraying your belief system .   \n",
              "2384  4611  You are in a loud minority , railing against t...   \n",
              "2385  9328           You bet your XXX that 'd make me happy .   \n",
              "2386  5225            you say `` f * * * the Constitution. ``   \n",
              "2387    68  Your answers were without content or meaning ....   \n",
              "\n",
              "                                                      r         s   q_reidx  \\\n",
              "2378  Different sciences . There are different spins...     AGREE   (0, 17)   \n",
              "2379                                  Well , they did .     AGREE   (0, 65)   \n",
              "2380  So doing a detailed study of something require...  DISAGREE  (0, 318)   \n",
              "2381  do you have proof of such a statement ? And wh...  DISAGREE   (0, 77)   \n",
              "2382             No ... and you do n't have to either .  DISAGREE   (0, 71)   \n",
              "2383  Yep . ( I 'm assuming that by `` belief system...     AGREE   (0, 37)   \n",
              "2384  Being in the minority or in the majority is ir...  DISAGREE   (0, 77)   \n",
              "2385  Well , first , I probably would n't bet my XXX...  DISAGREE   (0, 39)   \n",
              "2386  and gun nuts say f * * * the children when we ...  DISAGREE   (0, 38)   \n",
              "2387                                                  k  DISAGREE   (0, 68)   \n",
              "\n",
              "       r_reidx  q_sub_idx  r_sub_idx  \\\n",
              "2378  (0, 106)          0          0   \n",
              "2379   (0, 16)          0          0   \n",
              "2380  (0, 210)          0          0   \n",
              "2381  (0, 166)          0          0   \n",
              "2382   (0, 37)          0          0   \n",
              "2383  (0, 270)          0          0   \n",
              "2384   (0, 90)          0          0   \n",
              "2385  (0, 237)          0          0   \n",
              "2386   (0, 99)          0          0   \n",
              "2387    (0, 0)          0          0   \n",
              "\n",
              "                                                    s+r  \n",
              "2378  AGREE: Different sciences . There are differen...  \n",
              "2379                           AGREE: Well , they did .  \n",
              "2380  DISAGREE: So doing a detailed study of somethi...  \n",
              "2381  DISAGREE: do you have proof of such a statemen...  \n",
              "2382   DISAGREE: No ... and you do n't have to either .  \n",
              "2383  AGREE: Yep . ( I 'm assuming that by `` belief...  \n",
              "2384  DISAGREE: Being in the minority or in the majo...  \n",
              "2385  DISAGREE: Well , first , I probably would n't ...  \n",
              "2386  DISAGREE: and gun nuts say f * * * the childre...  \n",
              "2387                                        DISAGREE: k  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv(\"/home/shuxian109504502/AICUP/data/Batch_answers - test_data(no_label).csv\")\n",
        "test.tail()\n",
        "test[['q','r']] = test[['q','r']].apply(lambda x: x.str.strip('\\\"'))\n",
        "test.tail()\n",
        "def split_sen(data_):    \n",
        "    for i,(j,z) in enumerate(zip(data_[\"q\"], data_[\"r\"])):\n",
        "        # print(i, print(data_[\"q\"][i]))\n",
        "        if len(j.split(\" \")) > 200:\n",
        "            n = math.ceil(len(j.split(\" \"))/200)\n",
        "            tmp = j.split(\" . \")\n",
        "            n = math.ceil(len(tmp)/n)\n",
        "            data_[\"q\"][i] = [(\" . \").join(tmp[idx : idx + n]) for idx in range(0, len(tmp), n)]\n",
        "        else:   data_[\"q\"][i] = [j]\n",
        "        if len(z.split(\" \")) > 200:\n",
        "            n = math.ceil(len(z.split(\" \"))/200)\n",
        "            tmp = z.split(\" . \")\n",
        "            n = math.ceil(len(tmp)/n)\n",
        "            data_[\"r\"][i] = [(\" . \").join(tmp[idx : idx + n]) for idx in range(0, len(tmp), n)]\n",
        "        else:   data_[\"r\"][i] = [z]\n",
        "    return data_\n",
        "\n",
        "def re_idx(array):\n",
        "    idx_list = np.array([len(x) for x in array])+3\n",
        "    idx_list_ = np.cumsum(idx_list)\n",
        "    s_list = idx_list_ - idx_list\n",
        "    idx_list_ -= 4 \n",
        "    return [(x, y ,i) for i,(x,y) in enumerate(zip(s_list, idx_list_))]\n",
        "\n",
        "def re_pair(q, q_redix):\n",
        "    return [[a,b] for (a,b) in zip(q, q_redix)]\n",
        "\n",
        "test = split_sen(test)\n",
        "test[\"q_reidx\"] = test.apply(lambda x : re_idx(x[\"q\"]), axis=1)\n",
        "test[\"r_reidx\"] = test.apply(lambda x : re_idx(x[\"r\"]), axis=1)\n",
        "test[\"q\"] = test.apply(lambda x : re_pair(x[\"q\"], x[\"q_reidx\"]), axis=1)\n",
        "test[\"r\"] = test.apply(lambda x : re_pair(x[\"r\"], x[\"r_reidx\"]), axis=1)\n",
        "test = test.explode('q').reset_index(drop=True)\n",
        "test = test.explode('r').reset_index(drop=True)\n",
        "test[\"q_reidx\"] = test[\"q\"].apply(lambda x : (x[1][0], x[1][1]))\n",
        "test[\"q_sub_idx\"] = test[\"q\"].apply(lambda x : x[1][-1])\n",
        "test[\"q\"] = test[\"q\"].apply(lambda x : x[0])\n",
        "test[\"r_reidx\"] = test[\"r\"].apply(lambda x : (x[1][0], x[1][1]))\n",
        "test[\"r_sub_idx\"] = test[\"r\"].apply(lambda x : x[1][-1])\n",
        "test[\"r\"] = test[\"r\"].apply(lambda x : x[0])\n",
        "test[\"s+r\"] = test[\"s\"] + \": \" + test[\"r\"]\n",
        "test.tail(10)   #2387"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data_q = test['q'].tolist()\n",
        "test_data_r = test['s+r'].tolist()\n",
        "test_q_reidx = test['q_reidx'].tolist()\n",
        "test_r_reidx = test['r_reidx'].tolist()\n",
        "test_encodings = tokenizer(test_data_q, test_data_r, truncation=True, padding=True, max_length=512, return_offsets_mapping=True)\n",
        "test_offset_mapping = test_encodings[\"offset_mapping\"]\n",
        "test_encodings.pop(\"offset_mapping\")\n",
        "test_encodings.keys()\n",
        "test_dataset = qrDataset(test_encodings)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N3-L47N84Oh",
        "outputId": "af30e502-9d8e-47f0-f32b-6a9e69ba317f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:41<00:00,  7.21it/s]\n"
          ]
        }
      ],
      "source": [
        "# q_sub_output, r_sub_output, predict_pos = predict(test_loader)\n",
        "predict_pos = predict(test_loader, test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_sub, r_sub = [], []\n",
        "for i in range(len(predict_pos)):\n",
        "    q_s = test_offset_mapping[i][predict_pos[i][0]][0]\n",
        "    q_e = test_offset_mapping[i][predict_pos[i][2]][-1]\n",
        "    r_s = test_offset_mapping[i][predict_pos[i][1]][0]\n",
        "    r_e = test_offset_mapping[i][predict_pos[i][3]][-1]\n",
        "    q_pre_sen = test_data_q[i][q_s:q_e]\n",
        "    r_pre_sen = test_data_r[i][r_s:r_e]\n",
        "    q_sub.append(q_pre_sen)\n",
        "    r_sub.append(r_pre_sen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "couPbYb9obzZ"
      },
      "outputs": [],
      "source": [
        "test['q_sub'] = q_sub\n",
        "test['r_sub'] = r_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "fbXAY7PnombE",
        "outputId": "677dc5b5-91c8-4b83-f308-0fedff932b3d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "      <th>s</th>\n",
              "      <th>q_reidx</th>\n",
              "      <th>r_reidx</th>\n",
              "      <th>q_sub_idx</th>\n",
              "      <th>r_sub_idx</th>\n",
              "      <th>s+r</th>\n",
              "      <th>q_sub</th>\n",
              "      <th>r_sub</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6199</td>\n",
              "      <td>-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -...</td>\n",
              "      <td>If so , why do we still have apes , and why ar...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 120)</td>\n",
              "      <td>(0, 208)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: If so , why do we still have apes , ...</td>\n",
              "      <td>-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -...</td>\n",
              "      <td>If so , why do we still have apes , and why ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5807</td>\n",
              "      <td>There 's a lot of discussion there on that iss...</td>\n",
              "      <td>Of course . The makers of Expelled were within...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 180)</td>\n",
              "      <td>(0, 258)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: Of course . The makers of Expelled w...</td>\n",
              "      <td>There 's a lot of discussion there on that iss...</td>\n",
              "      <td>Of course . The makers of Expelled were within...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8487</td>\n",
              "      <td>`` It 's not helping . The guns these people h...</td>\n",
              "      <td>Oh , I would wager about like Mexico , about 8...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 375)</td>\n",
              "      <td>(0, 370)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: Oh , I would wager about like Mexico...</td>\n",
              "      <td>The guns these people have , they do n't regis...</td>\n",
              "      <td>Oh , I would wager about like Mexico , about 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1760</td>\n",
              "      <td>Shooting : 3 seriously injured in Auburn shoot...</td>\n",
              "      <td>Pickup strikes group of four youths | Houston ...</td>\n",
              "      <td>AGREE</td>\n",
              "      <td>(0, 434)</td>\n",
              "      <td>(0, 267)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>AGREE: Pickup strikes group of four youths | H...</td>\n",
              "      <td>This is America ! You do n't need no stinkin '...</td>\n",
              "      <td>Pickup strikes group of four youths | Houston ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6228</td>\n",
              "      <td>This is the argument concerning 'choice ' that...</td>\n",
              "      <td>I believe there is a point at which we ( socie...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 529)</td>\n",
              "      <td>(0, 144)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: I believe there is a point at which ...</td>\n",
              "      <td>This is the argument concerning 'choice ' that...</td>\n",
              "      <td>I believe there is a point at which we ( socie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6228</td>\n",
              "      <td>However , # $ % ( happens and a woman MUST be ...</td>\n",
              "      <td>I believe there is a point at which we ( socie...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(533, 967)</td>\n",
              "      <td>(0, 144)</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: I believe there is a point at which ...</td>\n",
              "      <td>a fetus is or will be human and an abortion do...</td>\n",
              "      <td>I believe there is a point at which we ( socie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3537</td>\n",
              "      <td>My point is , people are prepared to admit tha...</td>\n",
              "      <td>The bible is not a science textbook . I do not...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 589)</td>\n",
              "      <td>(0, 64)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: The bible is not a science textbook ...</td>\n",
              "      <td>My point is , people are prepared to admit tha...</td>\n",
              "      <td>The bible is not a science textbook . I do not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3874</td>\n",
              "      <td>Originally Posted by wayneinFL Each owns his o...</td>\n",
              "      <td>According to your logic , an infant would have...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 268)</td>\n",
              "      <td>(0, 173)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: According to your logic , an infant ...</td>\n",
              "      <td>The fetus does not have a positive right to su...</td>\n",
              "      <td>According to your logic , an infant would have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6223</td>\n",
              "      <td>Really , so you are totally knowable ? See abo...</td>\n",
              "      <td>Oh irrational-boy , please do try to understan...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 187)</td>\n",
              "      <td>(0, 796)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DISAGREE: Oh irrational-boy , please do try to...</td>\n",
              "      <td>Really , so you are totally knowable ? See abo...</td>\n",
              "      <td>Oh irrational-boy , please do try to understan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6223</td>\n",
              "      <td>Really , so you are totally knowable ? See abo...</td>\n",
              "      <td>This is why you MUST believe through faith , w...</td>\n",
              "      <td>DISAGREE</td>\n",
              "      <td>(0, 187)</td>\n",
              "      <td>(800, 1860)</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>DISAGREE: This is why you MUST believe through...</td>\n",
              "      <td>Really , so you are totally knowable ? See abo...</td>\n",
              "      <td>This is why you MUST believe through faith , w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                                  q  \\\n",
              "0  6199  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -...   \n",
              "1  5807  There 's a lot of discussion there on that iss...   \n",
              "2  8487  `` It 's not helping . The guns these people h...   \n",
              "3  1760  Shooting : 3 seriously injured in Auburn shoot...   \n",
              "4  6228  This is the argument concerning 'choice ' that...   \n",
              "5  6228  However , # $ % ( happens and a woman MUST be ...   \n",
              "6  3537  My point is , people are prepared to admit tha...   \n",
              "7  3874  Originally Posted by wayneinFL Each owns his o...   \n",
              "8  6223  Really , so you are totally knowable ? See abo...   \n",
              "9  6223  Really , so you are totally knowable ? See abo...   \n",
              "\n",
              "                                                   r         s     q_reidx  \\\n",
              "0  If so , why do we still have apes , and why ar...  DISAGREE    (0, 120)   \n",
              "1  Of course . The makers of Expelled were within...  DISAGREE    (0, 180)   \n",
              "2  Oh , I would wager about like Mexico , about 8...  DISAGREE    (0, 375)   \n",
              "3  Pickup strikes group of four youths | Houston ...     AGREE    (0, 434)   \n",
              "4  I believe there is a point at which we ( socie...  DISAGREE    (0, 529)   \n",
              "5  I believe there is a point at which we ( socie...  DISAGREE  (533, 967)   \n",
              "6  The bible is not a science textbook . I do not...  DISAGREE    (0, 589)   \n",
              "7  According to your logic , an infant would have...  DISAGREE    (0, 268)   \n",
              "8  Oh irrational-boy , please do try to understan...  DISAGREE    (0, 187)   \n",
              "9  This is why you MUST believe through faith , w...  DISAGREE    (0, 187)   \n",
              "\n",
              "       r_reidx  q_sub_idx  r_sub_idx  \\\n",
              "0     (0, 208)          0          0   \n",
              "1     (0, 258)          0          0   \n",
              "2     (0, 370)          0          0   \n",
              "3     (0, 267)          0          0   \n",
              "4     (0, 144)          0          0   \n",
              "5     (0, 144)          1          0   \n",
              "6      (0, 64)          0          0   \n",
              "7     (0, 173)          0          0   \n",
              "8     (0, 796)          0          0   \n",
              "9  (800, 1860)          0          1   \n",
              "\n",
              "                                                 s+r  \\\n",
              "0  DISAGREE: If so , why do we still have apes , ...   \n",
              "1  DISAGREE: Of course . The makers of Expelled w...   \n",
              "2  DISAGREE: Oh , I would wager about like Mexico...   \n",
              "3  AGREE: Pickup strikes group of four youths | H...   \n",
              "4  DISAGREE: I believe there is a point at which ...   \n",
              "5  DISAGREE: I believe there is a point at which ...   \n",
              "6  DISAGREE: The bible is not a science textbook ...   \n",
              "7  DISAGREE: According to your logic , an infant ...   \n",
              "8  DISAGREE: Oh irrational-boy , please do try to...   \n",
              "9  DISAGREE: This is why you MUST believe through...   \n",
              "\n",
              "                                               q_sub  \\\n",
              "0  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -...   \n",
              "1  There 's a lot of discussion there on that iss...   \n",
              "2  The guns these people have , they do n't regis...   \n",
              "3  This is America ! You do n't need no stinkin '...   \n",
              "4  This is the argument concerning 'choice ' that...   \n",
              "5  a fetus is or will be human and an abortion do...   \n",
              "6  My point is , people are prepared to admit tha...   \n",
              "7  The fetus does not have a positive right to su...   \n",
              "8  Really , so you are totally knowable ? See abo...   \n",
              "9  Really , so you are totally knowable ? See abo...   \n",
              "\n",
              "                                               r_sub  \n",
              "0  If so , why do we still have apes , and why ar...  \n",
              "1  Of course . The makers of Expelled were within...  \n",
              "2  Oh , I would wager about like Mexico , about 8...  \n",
              "3  Pickup strikes group of four youths | Houston ...  \n",
              "4  I believe there is a point at which we ( socie...  \n",
              "5  I believe there is a point at which we ( socie...  \n",
              "6  The bible is not a science textbook . I do not...  \n",
              "7  According to your logic , an infant would have...  \n",
              "8  Oh irrational-boy , please do try to understan...  \n",
              "9  This is why you MUST believe through faith , w...  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3890\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2016, 2016, 2016)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans_id, ans_q, ans_r = [], [], []\n",
        "for id in set(test[\"id\"]):\n",
        "    if id == 3890:\n",
        "        print(id)\n",
        "    frame = test[test[\"id\"] == id]\n",
        "    q_set =set(frame[\"q_sub_idx\"])\n",
        "    r_set =set(frame[\"r_sub_idx\"])\n",
        "    q_sub, r_sub = \"\", \"\"\n",
        "    if len(q_set) == 1:\n",
        "        q_sub = frame[\"q_sub\"].iloc[0]\n",
        "        if q_sub == \"\":\n",
        "            q_sub = frame[\"q\"].iloc[0]\n",
        "    else:\n",
        "        for idx in q_set:\n",
        "            # find max len by q_set to find in frame\n",
        "            q_frame = frame[frame[\"q_sub_idx\"] == idx]\n",
        "            max_idx = max(len(q) for q in q_frame[\"q_sub\"])\n",
        "            # print(\"q\", max_idx)\n",
        "            for q in q_frame[\"q_sub\"]:\n",
        "                if len(q) == max_idx:\n",
        "                    # print(q)\n",
        "                    q_sub += q\n",
        "                    # print(\"q_sub\", q_sub)\n",
        "                    break\n",
        "    if len(q_sub) == 0:\n",
        "        # print(frame)\n",
        "        if len(frame) == 1:\n",
        "            q_sub = frame[\"q_sub\"].iloc[0]\n",
        "        else:\n",
        "            q_sub = frame[\"q\"][frame[\"q\"].index[0]]\n",
        "            for idx, q in enumerate(frame[\"q\"][1:]):\n",
        "                if frame[\"q_sub_idx\"][frame[\"q\"].index[0]+idx+1] != frame[\"q_sub_idx\"][frame[\"q\"].index[0]+idx]:\n",
        "                    q_sub += q\n",
        "\n",
        "    if len(r_set) == 1:\n",
        "        r_sub = frame[\"r_sub\"].iloc[0]\n",
        "        if r_sub == \"\":\n",
        "            r_sub = frame[\"r\"].iloc[0]\n",
        "    else:\n",
        "        for idx in r_set:\n",
        "            # find max len by q_set to find in frame\n",
        "            r_frame = frame[frame[\"r_sub_idx\"] == idx]\n",
        "            max_idx = max(len(r) for r in r_frame[\"r_sub\"])\n",
        "            # print(\"r\", max_idx)\n",
        "            for r in r_frame[\"r_sub\"]:\n",
        "                if len(r) == max_idx:\n",
        "                    # print(r)\n",
        "                    r_sub += r\n",
        "                    # print(\"r_sub\", r_sub)\n",
        "                    break\n",
        "\n",
        "    if len(r_sub) == 0:\n",
        "        # print(idx)\n",
        "        if len(frame) == 1:\n",
        "            r_sub = frame[\"r_sub\"].iloc[0]\n",
        "        else:\n",
        "            r_sub = frame[\"r\"][frame[\"r\"].index[0]]\n",
        "            for idx, r in enumerate(frame[\"r\"][1:]):\n",
        "                if frame[\"r_sub_idx\"][frame[\"r\"].index[0]+idx+1] != frame[\"r_sub_idx\"][frame[\"r\"].index[0]+idx]:\n",
        "                    r_sub += r\n",
        "\n",
        "    ans_id.append(id)\n",
        "    ans_q.append('\"'+q_sub+'\"')\n",
        "    ans_r.append('\"'+r_sub+'\"')\n",
        "\n",
        "len(ans_id), len(ans_q), len(ans_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "for q in ans_q:\n",
        "    if q == '\"\"':\n",
        "        print(q)\n",
        "\n",
        "for r in ans_r:\n",
        "    if r == '\"\"':\n",
        "        print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>q</th>\n",
              "      <th>r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\"I got a good idea . however , they do tend to...</td>\n",
              "      <td>\"By your own admission you havenÃ‚ â€™ t 'hung ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>\"Be sure to give your guns a big fat kiss toni...</td>\n",
              "      <td>\"Actually , they did n't . The whole tragedy w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>\"One of the biggest arguments against gun cont...</td>\n",
              "      <td>\"Not quite . To be more correct regarding gove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>\"compare the ' B ' specimen in your fossil lin...</td>\n",
              "      <td>\"At your service : Comparison I could 've just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>\"There are some incedents that are beyond your...</td>\n",
              "      <td>\"Well yes .\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>8186</td>\n",
              "      <td>\"It seems that you would be willing to grant t...</td>\n",
              "      <td>\"Sorry to hear you lost an hour 's worth of wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012</th>\n",
              "      <td>8187</td>\n",
              "      <td>\"Waiting until they are born likely gives them...</td>\n",
              "      <td>\"I think they have more of a chance becasue , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013</th>\n",
              "      <td>8188</td>\n",
              "      <td>\"The government was right to tighten up the la...</td>\n",
              "      <td>\"So those who will allow more gun control , sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014</th>\n",
              "      <td>8189</td>\n",
              "      <td>\"would you have a problem calling a guy marrag...</td>\n",
              "      <td>\"Yes , I would . The term is not simply just w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015</th>\n",
              "      <td>8190</td>\n",
              "      <td>\"This sedimentary layer would show signs of be...</td>\n",
              "      <td>\"It has been shown in evidence all around the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2016 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                                  q  \\\n",
              "0        1  \"I got a good idea . however , they do tend to...   \n",
              "1        2  \"Be sure to give your guns a big fat kiss toni...   \n",
              "2        3  \"One of the biggest arguments against gun cont...   \n",
              "3        4  \"compare the ' B ' specimen in your fossil lin...   \n",
              "4        5  \"There are some incedents that are beyond your...   \n",
              "...    ...                                                ...   \n",
              "2011  8186  \"It seems that you would be willing to grant t...   \n",
              "2012  8187  \"Waiting until they are born likely gives them...   \n",
              "2013  8188  \"The government was right to tighten up the la...   \n",
              "2014  8189  \"would you have a problem calling a guy marrag...   \n",
              "2015  8190  \"This sedimentary layer would show signs of be...   \n",
              "\n",
              "                                                      r  \n",
              "0     \"By your own admission you havenÃ‚ â€™ t 'hung ou...  \n",
              "1     \"Actually , they did n't . The whole tragedy w...  \n",
              "2     \"Not quite . To be more correct regarding gove...  \n",
              "3     \"At your service : Comparison I could 've just...  \n",
              "4                                          \"Well yes .\"  \n",
              "...                                                 ...  \n",
              "2011  \"Sorry to hear you lost an hour 's worth of wo...  \n",
              "2012  \"I think they have more of a chance becasue , ...  \n",
              "2013  \"So those who will allow more gun control , sh...  \n",
              "2014  \"Yes , I would . The term is not simply just w...  \n",
              "2015  \"It has been shown in evidence all around the ...  \n",
              "\n",
              "[2016 rows x 3 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans = pd.DataFrame({\"id\": ans_id, \"q\": ans_q, \"r\": ans_r})\n",
        "# pd.set_option('display.max_colwidth', -1)\n",
        "ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "ans.to_csv(\"submission_roberta_3e5_check.csv\", index=False, encoding=\"utf-8\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.15 ('AICUP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c9682674024c9e344d97ee54cc2a0dde35ce2d0c3e9b7257167e51bb860858f7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
